{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "\n",
    "train_data = unidecode.unidecode(open('sherlock.txt').read()) # load the text file, reading it\n",
    "vocab = string.printable # use all printable string characters as vocabulary\n",
    "vocab_length = len(vocab) # vocabulary length\n",
    "data_len = len(train_data) # get length of training data\n",
    "\n",
    "# utility function\n",
    "# get_batch utility function will randomly sample a batch of data of size k from a text corpus\n",
    "def get_batch(text_corpus, batch_size=100):\n",
    "    start = random.randint(0, data_len-batch_size)\n",
    "    end = start + batch_size + 1\n",
    "    return text_corpus[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers=1, rnn_type='gru'):\n",
    "        \"\"\"rnn class making\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size # this is our vocabulary size, i.e 100\n",
    "        self._embedding_size = embedding_size # this is our embedding size, i.e the output size of embedding our sparse\n",
    "        # matrix, set at say 50\n",
    "        self._hidden_size = hidden_size # hidden size for the hidden rnn\n",
    "        self._n_layers = n_layers\n",
    "        \n",
    "        # create layers. If rnn_type is gru, use gru\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, n_layers)\n",
    "        else:\n",
    "            raise NotImplementedError # this is to be implemented, for example replace with lstm\n",
    "        self.h2o = nn.Linear(hidden_size, vocab_size) # the hidden to output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=0) # numerically stable implementation of log of softmax. Need the log-softmax for\n",
    "        # computing the cross entropy log loss\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \"\"\"given an x and a hidden h, forward pass through our network. Our final output should be a softmax prediction\n",
    "        over all the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            x: input of shape [seq_len] x will be a long tensor of size seq_len, essentially a list of integers ranging from\n",
    "            0 to 100, i.e x = [0, 5, 24, 0, 66]\n",
    "            h: h_0** of shape `(num_layers * num_directions, batch, hidden_size)`\n",
    "        \n",
    "        \"\"\"\n",
    "        # step 1, get sequence length\n",
    "        seq_len = x.size()[0]\n",
    "        # step 2. pass our input through our embedding layer, and get the output \"embed\", reshape it via view to get\n",
    "        # it ready for the rnn layer\n",
    "        embed = self.embedding(x).view(seq_len, 1, -1) # rnn takes input of shape [seq_len x batch_size x input_dim]\n",
    "        # step 3. forward pass our embed through our rnn layers, make sure to pass in hidden as well\n",
    "        rnn_out, hidden = self.rnn(embed, h) # compute the rnn output\n",
    "        # step 4: using our rnn output, pass it through the i2o(input to output) linear layer (remember to reshape to 2D)\n",
    "        # and get the non-normalized output prediction\n",
    "        prediction = self.h2o(rnn_out.view(seq_len,-1))\n",
    "        # step 5: normalize our prediction by taking the log_softmax\n",
    "        log_softmax = self.softmax(prediction)\n",
    "        # return log softmax prediction and hidden\n",
    "        return log_softmax, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self._n_layers, 1, self._hidden_size)\n",
    "    \n",
    "    \n",
    "def logprob_to_words(logprob, vocab):\n",
    "    \"\"\"given a sequence of logprobs from a network and a vocabulary, turn the logprob into words\n",
    "    \n",
    "    \"\"\"\n",
    "    seq_len = logprob.shape[0]\n",
    "    max_val, max_idx = logprob.max(dim=1)\n",
    "    txt = ''\n",
    "    for item in max_idx:\n",
    "        txt+=(idx_to_word[int(item)])\n",
    "    \n",
    "    return txt\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch_1524585239153/work/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6789e07f842e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# step 1: create our network and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# step 2: create a training batch of data, size 101, format this data and convert it to pytorch long tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch_1524585239153/work/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "# practice tests\n",
    "\n",
    "# step 1: create our network and optimizer\n",
    "net = RNN(100, 100, 100)\n",
    "net.cuda()\n",
    "optim = torch.optim.Adam(net.parameters(),lr=1e-4)\n",
    "# step 2: create a training batch of data, size 101, format this data and convert it to pytorch long tensors\n",
    "dat = get_batch(train_data,100)\n",
    "dat = torch.LongTensor([vocab.find(item) for item in dat])\n",
    "# step 3: convert our dat into input/output\n",
    "x_t = dat[:-1].cuda()\n",
    "y_t = dat[1:].cuda()\n",
    "# step 4: initialize hidden state and forward pass\n",
    "ho = net.init_hidden().cuda()\n",
    "log_prob, hidden = net.forward(x_t, ho)\n",
    "# step 5: import our loss and compute the loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = -loss_func(log_prob, y_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\n",
      " epoch 0, loss:5.056777000427246 \n",
      "\n",
      "sample speech:\n",
      " L&T38~3TUucP)>U4ia55k)s[F8Zk.q9OdPHQdYP)>UqSn4\tnyULL?3,,T?x#P\f",
      "H, H\f",
      "\f",
      "\f",
      "\f",
      "^>~\f",
      ",6WR>B9\f",
      "p,HF>U]P^UH`LUP6]zPT?x#Pwld\f",
      "4'g%Z\f",
      "\f",
      ",Z\f",
      "7aauuakTn\"\u000b",
      "9WO.S~L5UG. 9KdLUGG\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 100, loss:4.560878276824951 \n",
      "\n",
      "sample speech:\n",
      " d.buubhe bolng ngg\n",
      "\"\"\n",
      "\n",
      "u.whhuwon  wnQ  .ba\n",
      "bn..nenybnhwhe bourrr.barr+er wa\n",
      "bongrban \"er.\n",
      "bulnd Swugwhe bhen.  woi. wolon  e bonerbouzgwzlthwnE  .\"\"er\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 200, loss:4.3668437004089355 \n",
      "\n",
      "sample speech:\n",
      " r,erg.Ihu\"\"ug\n",
      "I \n",
      "Iourbhndwxd$u Inhbngborver Iaryywnd.Iouu \"I \n",
      "Iourbong.Hour\"hndgu  Ihne baobn ener Iaobeu  Ind IoxverbnhIhubougeborv.yn,..\"rgdy\n",
      "\"I Ine\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 300, loss:4.305738925933838 \n",
      "\n",
      "sample speech:\n",
      " `e.l..In d Iex$ng,Hnbop^hvngwngwherbooud.Iounebhx.d\"vlywhe boou e .wf boi\"Ioxecyydboh\"Ioodd. Ihshewhe bx0hrrhnngwf \"ac.bouv \"Ioc beuu .\"Iac bouke.\"Iac\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 400, loss:4.168686389923096 \n",
      "\n",
      "sample speech:\n",
      " \"\"I#pkeCoum,wnhwpp Mher.\"\n",
      "\n",
      "\"Iher.dwn bnmhevngwn wavl.kov..bocgd \n",
      "Huvd.S  Hoovdeng,wfer.wy \"uov..ev?\"HI  wn bopxbn wocgker,.r.wn waai--\"\n",
      "\n",
      "\"Ihev whslywo\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 500, loss:4.143948078155518 \n",
      "\n",
      "sample speech:\n",
      " u,Saavkd\"Ihaumf Saich Ixg.r.Hngwhe \"amlgk,\n",
      "kug..\"Ipg.glMn Iov..Iixwuxkve,bhe whxeeu.Mf She wiclyvkd\"\"nd.Hhe woul,,Saurd HoooHn Iiv Mnppendk.\"Iou.Mixwn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 600, loss:4.061468124389648 \n",
      "\n",
      "sample speech:\n",
      " eHh~bexwoug.\"aptevd.Hpdhnlb wouexbuvk,\"\n",
      "Texyy Iixwh[l-,,Hn.Ifer.\"Ind Iixwav Ooc[x,Hn.Inlywppburmogk,y,\"Tish Inmoox.e.evdMnlywoxv?.Sngwisn?ng,\"\"het Iix\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 700, loss:4.025486946105957 \n",
      "\n",
      "sample speech:\n",
      " k?rwax..wgtywn irn.,Lngwhec boplygkjyvggd.\"\n",
      "\n",
      "\"The ,Mougbhuds?Lor.-nexfyx\"Hougbhuds?Luev-,wavd?,Sexrr.dw wom\"'\n",
      "\"YNe Ioxeibngy \"In jn Lhuld Locexbougbnd\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 800, loss:4.009561538696289 \n",
      "\n",
      "sample speech:\n",
      " ng?Horkkj Bave \"uxx?\"Tnd\n",
      "Iis In eng,Hh bexwnlyu,,,Ih boumfp?Iou.Ioce.Syngl.d.\"\"hexCoxvyMorlyu,Iisd?r,Ih buxxwn\n",
      "jhe\n",
      "Pynt.ng,HocxkdSixedwn Ihe\n",
      "Ouoxtdwf \n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 900, loss:3.709958076477051 \n",
      "\n",
      "sample speech:\n",
      " Chet Iiick Mn bove.,Mh mfd.bf jhe Cuar?r,bf jhe C}ogng\"\n",
      "\n",
      "\"Yhxpk? M wheng,\"\n",
      "\n",
      "\"YVwas bow.jnd Lnpbow.jup..biick 'Sut I wnox-Chet Ihe Cuec-byst.bave bndwn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1000, loss:4.053675174713135 \n",
      "\n",
      "sample speech:\n",
      " n^her.s \"Hhmeq5vw.edwup:eiryywav bern?joxer.bybhmp.,bizhry.\"Houmfe,bxlpd M whesg,'\n",
      "\n",
      "\"Yhumou Commfg jyph jn jolknd..\"\n",
      "\n",
      "\"Ynd^er.bowd,\"Hird,bnd b whar.bn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1100, loss:4.174521446228027 \n",
      "\n",
      "sample speech:\n",
      " \"evdLhep\n",
      "'S\"Yoic/Cr0\"TooEz=t}pk\n",
      "wnpMiry.Snd$nng.Mh~Romd.ly.Mou Cn qh~Rhix\n",
      "er.\" wheugd?Mf Moeugd?Mow Inkkxp.Mnqtx.rl?nn,Mhich Iav\n",
      "qexx,Mf -r,d,\"h~byxqn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1200, loss:3.8553478717803955 \n",
      "\n",
      "sample speech:\n",
      " +Mas jx\n",
      "r.Moamt;,Hnd Ias buozk.Hoaz!Ircp;bheogt.ng.Mft Iive;bhe\n",
      "KuclyJf thle;wurozg.dwuc:.\"T\n",
      "Bav Ioupswh jhe\n",
      "Koudeictnn,Mhet IirKav Iooue-n.Mn y,d,\"Mn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1300, loss:3.8094356060028076 \n",
      "\n",
      "sample speech:\n",
      " eCoxpryz?dbhs Lurzkng.wn Liu?eng..bn Lhe ch~-,\"\n",
      "\"YherKrozk,burzlybnxx-.biry.basht\"\n",
      "Sarcoxpzk-d,bn qarcuv^Lown,\"\n",
      "Tt wn qf jnpln..bde,whet In Iueuld bow\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1400, loss:4.10091495513916 \n",
      "\n",
      "sample speech:\n",
      " \"Mnxn?-icher,\"Muxv?.bivd?r,LoozpngtyKnort,\"Mn qn whs Gautizle,wh~moudexne;\"T\n",
      "'Bas quxg.ryk.woazkbk.d.whe\n",
      "Koctlqothre;bnl.r.itiry,Gn ixyid,Ln |rf\n",
      "\"Mnd\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1500, loss:3.7031071186065674 \n",
      "\n",
      "sample speech:\n",
      " u,,r,bhrKyz,boupswh qnqoudeez:nn,\"\n",
      "\n",
      "\"Yhxl, Mazh.n, 'Shid?Iolmes.bhir Ift.biztzen.Mav Iixt-Ipp MNhit howqourCyze;wf tx.wnly\"\n",
      "\n",
      "\"YtCaze;wow eng Mf tx.\"\n",
      "S\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1600, loss:4.0296807289123535 \n",
      "\n",
      "sample speech:\n",
      " x,Mhs Iexuunk,,Hh Rrxg.Hy Soozxt,s GnkeCn Lyxg.\"Ind Ih~Rrx.In=y,Goom Mhe Cuaxtdwf Lppeuk.\"TE Casl?d,Shazeey,Snd In Lhxl??e,Sor Ihme;wor Pyzgser.Mpdizl\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1700, loss:3.8337085247039795 \n",
      "\n",
      "sample speech:\n",
      " -zier,Mispbnxg.y.Cfethe Koould?d.\"\n",
      "\"Y  Cou Cixve;qn.wh bnqoml..wf tiz?wh qoaxvkwhe Kaz?er,wpo \n",
      "Said\n",
      "bax MNf Loml..dwourCozkLavk y,Cneun,brzlygkz..\"Hn\f",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1800, loss:3.6411311626434326 \n",
      "\n",
      "sample speech:\n",
      " ng,L000.waulg?.wnpznk?fndqxpomt.ze;foppqor che e\"\n",
      "\n",
      "\"Y wauld boz,bhrg\"\n",
      "\n",
      "\"Yhet Ihuld be,qpdoxk.;.bk,\"Hhe&m!wheug:b??whtlybouerywhe Caz%er,\"\n",
      "nd Lhe e wn \n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 1900, loss:3.801645517349243 \n",
      "\n",
      "sample speech:\n",
      " \f",
      "eick?2h Uhe\n",
      "p\"To Cas Rnqpxg.lykkbyz?\"Moxx,kd2nd\n",
      "2uizk--hxpax;d.\"Miry,jorrdcewg^;r,Mhir IaxCask2nd0.-\"Mnd\n",
      "2f t qyrt.Kexhzeng.jovtlntneon,\"Yotkng.2nlyw\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2000, loss:3.922783613204956 \n",
      "\n",
      "sample speech:\n",
      " ^Knqhvdyng -tacs\"Yt wivk!.Mn qt\n",
      "qhe\n",
      "\n",
      "Cnlay,;woxc.jhe\n",
      "n!boxd-l-bdbisd?ng 2f=jh~e!?2exor,;2he\n",
      "pCiir Ihrzkyng.2pon?jhe\n",
      "n!byxt.zn.\"Tou Chxvwaw,Cuizh-y\n",
      "Goe\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2100, loss:3.85209059715271 \n",
      "\n",
      "sample speech:\n",
      " eChs cxpre.\"Mnd\n",
      "Lnqowhr2or qy.qizkGpon?Mhe\n",
      "Cavl,bhkle,\"Y'Cav buid,Hh~GereCizt?boght,\"Mn\n",
      "2umk.u,wnd Low,wn\n",
      "2nd-!;\"Mhet In\n",
      "Lue Kav byzkizd,by\n",
      "KuukGnl,Ky\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2200, loss:3.69142746925354 \n",
      "\n",
      "sample speech:\n",
      " C Cmpqemky.bxgugh;Mh~mhllybhit I Cnow,\"\n",
      "\n",
      "\"Yher?\"Muoz.\"Muz.2own?\"Mnd Liv,2pkjirrkCn.2or qhe e;wnk;1uxerylyb0zng..Hfethich I Cast boudir;;Hoet I Cspqurz\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2300, loss:3.8254036903381348 \n",
      "\n",
      "sample speech:\n",
      " +MnkLow,eng.Rxlp;wav bxerywow?.\"T''2nqauk,\"M whrned,3fer,wnqov,Civk,2nd Lov!by\n",
      "Kuft-wh-myze;1pobor qhe Kuzk?\"Ynd,Khs\n",
      "boung.1hrl,1hin?Irkarkee,Cizd,Iem\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2400, loss:3.729355573654175 \n",
      "\n",
      "sample speech:\n",
      " r\"Ttqogebn?\"Mr.\n",
      "Holmes,\"Wthauld bowqn?2nozn!\"Iorp-y.Knkq0qave 1hzgid,\"\" have 1ia,jnqigt;qf jyzk!.--u?wh-bn\n",
      "u?;2or qn?\"Yut 2het Iy\n",
      "Krzkycpould!be?qxciz\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2500, loss:3.931410312652588 \n",
      "\n",
      "sample speech:\n",
      " ch\"Yhe\n",
      "e wnk?1oxiryl-b0cply.3nq2he\n",
      "Koxchtlug..uk.\"Maunder,\"Mnd Inpn?-Rhe\n",
      "?Crgt;18rked.\"Mha\n",
      "poysht?d,wf Lhe\n",
      "Koxchtlug.ng.Sokk.u,.d,\"MhiuGexycdedqnq2am\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2600, loss:3.8174238204956055 \n",
      "\n",
      "sample speech:\n",
      " Cexpzn?.\"Mhe\n",
      "e\n",
      "or,?\"Mh Loxtlue;,j0it It\n",
      "q0sdyr,wu\n",
      "Khen\n",
      "q5xvpb?2hi Gaich?,wpon?Luuk!.nb?2lze?.fnd\n",
      "jrosor;.w0rkdng fnqyzt-Hh-Lueunng-fim\n",
      "qock,\"Ynd\n",
      "Ier ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\n",
      " epoch 2700, loss:3.624995231628418 \n",
      "\n",
      "sample speech:\n",
      "  ur,brxp,d'\n",
      "\n",
      "aid!ber\n",
      "\n",
      "\"Qy Krxp.d\"\n",
      "IherKaz?2uxp?d,thzn-ezhd-\"\n",
      "\"Qos,\"Htwas Khozreng-bfey Kavl-1nd2iug:bnauth-br.\n",
      "Hor?y-wuze!, bhi K0s Knqaxplx;wf tour b\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2800, loss:4.041996479034424 \n",
      "\n",
      "sample speech:\n",
      " hepk-Hir Cuizh-Holpzgngdfxssw0s Ihet Lnl.eugh;Mirq0ud!&hme!&oyk-Homrh Mhck Mn Iirqav bhrzkyd.Sh-Lh-,,3n \"Mirqav bfeLovnh.r.w8mlyck2ow:2ovk--zd\"\n",
      "\"Yolee\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 2900, loss:3.864773750305176 \n",
      "\n",
      "sample speech:\n",
      " h-bexuyzk.3n.\"Mn?wn qflezus:&het IirKav\n",
      "bixt;&or !;nht,&ow,whetkLor ed,y.\"\"hich\n",
      "In\n",
      "qnqoxt-zge.&8opk.&f Llq0xk!d?ng-Sothre.\"Tn\f",
      "the Kfher,3avd,\"MirKav\n",
      "q\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3000, loss:3.7153382301330566 \n",
      "\n",
      "sample speech:\n",
      " uk.;n,\"Mnd\n",
      "I0wav bufulg&h-Giue!fhet\n",
      "Lhes qorl:lfiv buzk;d-Snny,Goom Mhe Kozpny.\"Mnd\n",
      "Ihet\n",
      "Is?wav bxg.r.30th Mhe Kizk?&oft;.gion?\"T'wav bufulg&h-Ghke;&o\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3100, loss:3.6393823623657227 \n",
      "\n",
      "sample speech:\n",
      " ul:d;ed.bh-bexquazn?d;Mhth Moox:,Mu0wk.\"Tn\f",
      "jorlyw,ng-bim.bhe\n",
      " Gorld Ihe\n",
      "Koxrybyug.bqaex?hid Mfr!Ipon?Mhe\n",
      "crivk;byxtze;&he\n",
      "K0zki\"The\n",
      "Kaxv!biv\n",
      "bexn?Muxl\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3200, loss:3.880709648132324 \n",
      "\n",
      "sample speech:\n",
      " hkk.\"Mnd\n",
      "Lozglly\n",
      "Gefule?&nqequr.id.Sf\f",
      "L d2xer?ng-&0zer.2n 1omk.u?\"Tn\f",
      " woykGy Kxvzir,2hth d Mh-bive K qhqvzd,wf L kezkyd;wpon?Mufuizg.Sn Lhe\n",
      "Ca0?iu^nyn\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3300, loss:3.7426114082336426 \n",
      "\n",
      "sample speech:\n",
      " Cav\n",
      "b azn;2nd\n",
      "I azn;jh-boup!&h-by?2nd\n",
      "Inplauk;&y?2h-boxe;&is.jnd2nvence.&pon?Sis\n",
      "jnlyw,kge.\"Mhet IirCaxht?&hx?ly\n",
      "Kis\n",
      "qoxlh.KffLiugug.\"TerKhozd-byrk;&h\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3400, loss:3.8330764770507812 \n",
      "\n",
      "sample speech:\n",
      " d'K0Cxc-rreur.d,Sh-khmld,Iim.jpon?the\n",
      "C0zzg?\"Mut Ii]qllay,;joagg;;tnay,bh-khme;2fher.wh--ne\"Mpdollbn\n",
      "7izk?b0Coze;&n?wferyJnqjocpezr-\"Y' was Kow?wos.wh\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3500, loss:3.6145997047424316 \n",
      "\n",
      "sample speech:\n",
      " nkd&hich Ie wav bizd!byfpze;&im qoazr-\"\n",
      "You wave jowglhy.,,Sirrd,Sf the Cuqv--&0vkn?.?\"\n",
      "\n",
      "\"QWn\f",
      ";2f jhe Cart?10oskzns:M0zlygkM0zeid.;nn?,wf jhe Cxplzyd \n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3600, loss:3.739315986633301 \n",
      "\n",
      "sample speech:\n",
      " +\"Mu Khe\n",
      "C0s!\"Ln qemher.Snqhoz^-&fc;\"Tou C0tl,brxg,H0zkb!ydy&ozk?;\"Mn\n",
      "Jourjomkiri.&y\n",
      "Knqoxt\"Mnq1ld0wer,Snq1Y00\"Mnd\n",
      "Ihe\n",
      "e?w0s K5me??ing-Sf jhe\n",
      "Cpmk-&n \n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3700, loss:3.790894031524658 \n",
      "\n",
      "sample speech:\n",
      " r:ng-&rzll-.\"Yhes KarKave!Kot,wyfn?jrwzg-&rr 2ome?&hmp.\"Mnd\n",
      "ItqLfker,&h-kirl,&p.KnqLft bfenybizn?.w0rCxvckid-tnqa:eegg-nhPuossi\"Thes Kuospi\"Mn I0Cave!\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3800, loss:3.8303475379943848 \n",
      "\n",
      "sample speech:\n",
      " enlzg!&h bou  jxvk.\"Y'wmp1nkvtbd,d,&olm-r:\"Mf Lhe Cuzk-ng-&ozke&f Lolm-r:&8cqrrer?.n?\"Mf L8eeekyedd-yd&treqt?\"\n",
      "\n",
      "\"herK0zp?&hs Inqerd-&hrl,&now, Jh-bp.I\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 3900, loss:3.8578951358795166 \n",
      "\n",
      "sample speech:\n",
      " xqorl-iz^\"Yhe\n",
      "e?wnk &oxiryl-b0cplydpnq1he\n",
      "Coxvhtour..uk.\"Miu,ver,\"Mnd\n",
      "Ilpn?-&he\n",
      "?&rss:&8rner.\"Mhe\n",
      "Coyght!r,&f Lhe\n",
      "Coxvhtour.ng-&izk-u,'d,\"Mhi Kuxozve;\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 4000, loss:3.6853246688842773 \n",
      "\n",
      "sample speech:\n",
      " u -\"\n",
      "\n",
      "\"Qut Iew,\"\n",
      "\n",
      "\"Qheeught1he Kqi:ycht,\"HhlKuoll-bumk?&uq?&iw,&irwaz?le;-&n.\"\n",
      "He ju0zg;&is.:lf.&po1fcyrthe Keum,\"HOn! Mos. 'sa Koizd- MVer:?s:qhayjxg\n",
      "****************************************************************************************************\n",
      "\n",
      " epoch 4100, loss:3.780850887298584 \n",
      "\n",
      "sample speech:\n",
      " zg-&n?whm\"\n",
      "\n",
      "\"Qx0rexpdy.&0m\"H'womdliziryt!;&ourw0skey.\"\n",
      "\n",
      "\"Uxs?-by;&eu:;&nq2is qhozzbl.&nd 1yxe.&im qaxr-&h~Kivk!\"HOh0, 'sa pp0ozd-d-\"MVours:dwivk!ng-&n\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-93d4fa2a966b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# print the loss for every kth iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_yes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch-0.4/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "seq_batch_size = 150\n",
    "print_yes = 100\n",
    "loss_func = torch.nn.functional.nll_loss\n",
    "\n",
    "# step 1: create our network and optimizer\n",
    "net = RNN(100, 100, 100)\n",
    "optim = torch.optim.Adam(net.parameters(),lr=5e-4)\n",
    "\n",
    "# lets see if we can overfit\n",
    "dada = get_batch(train_data, 100)\n",
    "dada = torch.LongTensor([vocab.find(item) for item in dada])\n",
    "x_dada = dada[:-1]\n",
    "y_data = dada[1:]\n",
    "\n",
    "# main training loop:\n",
    "for epoch in range(epochs):\n",
    "    dat = dat = get_batch(train_data,seq_batch_size)\n",
    "    dat = torch.LongTensor([vocab.find(item) for item in dat])\n",
    "    # pull x and y\n",
    "    x_t = dat[:-1]\n",
    "    y_t = dat[1:]\n",
    "    # initialize hidden state and forward pass\n",
    "    hidden = net.init_hidden()\n",
    "    logprob, hidden = net.forward(x_t, hidden)\n",
    "    loss = loss_func(logprob, y_t)\n",
    "    # update\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print the loss for every kth iteration\n",
    "    if epoch % print_yes == 0:\n",
    "        print('*'*100)\n",
    "        print('\\n epoch {}, loss:{} \\n'.format(epoch, loss))\n",
    "        print('sample speech:\\n', logprob_to_words(logprob, vocab))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -6.1075,  -6.1198,  -6.2239,  ...,  -6.1796,  -6.2226,\n",
      "          -6.3669],\n",
      "        [ -6.0468,  -6.2410,  -6.0143,  ...,  -6.6546,  -6.3249,\n",
      "          -6.3432],\n",
      "        [ -6.3289,  -6.1807,  -6.2836,  ...,  -5.9856,  -6.4997,\n",
      "          -6.5055],\n",
      "        ...,\n",
      "        [ -6.5863,  -6.1425,  -6.3313,  ...,  -6.8871,  -6.0193,\n",
      "          -6.0095],\n",
      "        [ -5.8683,  -6.2851,  -6.3315,  ...,  -6.6275,  -6.6342,\n",
      "          -6.0157],\n",
      "        [ -5.7948,  -6.1305,  -6.5514,  ...,  -6.1083,  -6.3550,\n",
      "          -6.2403]])\n",
      "tensor([ 21,  14,  94,  10,  23,  13,  94,  22,  30,  12,  17,  94,\n",
      "         21,  14,  28,  28,  94,  10,  22,  18,  10,  11,  21,  14,\n",
      "         75,  94,  55,  17,  14,  34,  94,  10,  27,  14,  94,  10,\n",
      "         94,  22,  24,  28,  29,  96,  30,  23,  25,  21,  14,  10,\n",
      "         28,  10,  23,  29,  94,  12,  24,  30,  25,  21,  14,  73,\n",
      "         94,  11,  30,  29,  94,  15,  24,  27,  29,  30,  23,  10,\n",
      "         29,  14,  21,  34,  94,  44,  94,  28,  25,  14,  23,  13,\n",
      "         94,  22,  24,  28,  29,  94,  24,  15,  94,  22,  34,  94,\n",
      "         29,  18,  22,  14,  94,  18,  23,  94,  29,  17,  14,  96,\n",
      "         23,  30,  27,  28,  14,  27,  34,  94,  10,  23,  13,  94,\n",
      "         22,  34,  94,  24,  32,  23,  94,  27,  24,  24,  22,  73,\n",
      "         94,  32,  17,  18,  12,  17,  94,  10,  27,  14,  94,  23,\n",
      "         14,  33,  29,  94,  29,  24,  94,  14,  10,  12,  17,  94,\n",
      "         24,  29,  17,  14,  27,  94,  18,  23,  94,  24,  23,  14,\n",
      "         96,  12,  24,  27,  23,  14,  27,  94,  24,  15,  94,  29,\n",
      "         17,  14,  94,  11,  30,  18,  21,  13,  18,  23,  16,  75,\n",
      "         96,  96,  63,  41,  24,  27,  94,  29,  32,  24,  94,  13,\n",
      "         10,  34,  28,  94,  10,  15,  29,  14,  27,  94,  22,  34,\n",
      "         94,  10,  27,  27,  18,  31,  10,  21,  94,  10,  29,  94,\n",
      "         29,  17,  14,  94,  38,  24,  25,  25,  14,  27,  94,  37,\n",
      "         14,  14,  12,  17,  14,  28,  94,  22,  34,  94,  21,  18,\n",
      "         15,  14,  94,  32,  10,  28,  96,  31,  14,  27,  34,  94,\n",
      "         26,  30,  18,  14,  29,  78,  94,  24,  23,  94,  29,  17,\n",
      "         14,  94,  29,  17,  18,  27,  13,  73,  94,  48,  27,  28,\n",
      "         75,  94,  53,  30,  12,  10,  28,  29,  21,  14,  94,  12,\n",
      "         10,  22,  14,  94,  13,  24,  32,  23,  94,  19,  30,  28,\n",
      "         29,  94,  10,  15,  29,  14,  27,  96,  11,  27,  14,  10,\n",
      "         20,  15,  10,  28,  29,  94,  10,  23,  13,  94,  32,  17,\n",
      "         18,  28,  25,  14,  27,  14,  13,  94,  28,  24,  22,  14,\n",
      "         29,  17,  18,  23,  16,  94,  29,  24,  94,  17,  14,  27,\n",
      "         94,  17,  30,  28,  11,  10,  23,  13,  75,  96,  96,  63,\n",
      "         68,  50,  17,  73,  94,  34,  14,  28,  73,  68,  94,  28,\n",
      "         10,  18,  13,  94,  17,  14,  73,  94,  29,  30,  27,  23,\n",
      "         18,  23,  16,  94,  29,  24,  94,  22,  14,  73,  94,  68,\n",
      "         32,  14,  94,  10,  27,  14,  94,  31,  14,  27,  34,  94,\n",
      "         22,  30,  12,  17,  94,  24,  11,  21,  18,  16,  14,  13,\n",
      "         94,  29,  24,  96,  34,  24,  30,  73,  94,  48,  18,  28,\n",
      "         28,  94,  43,  30,  23,  29,  14,  27,  73,  94,  15,  24,\n",
      "         27,  94,  15,  10,  21,  21,  18,  23,  16,  94,  18,  23,\n",
      "         94,  32,  18,  29,  17,  94,  24,  30,  27,  94,  32,  17,\n",
      "         18,  22,  28,  94,  28,  24,  94,  15,  10,  27,  94,  10,\n",
      "         28,  94,  29,  24,  94,  12,  30,  29])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_batch = get_batch(train_data, 500)\n",
    "test_batch =torch.LongTensor([vocab.find(item) for item in test_batch])\n",
    "\n",
    "x_test = test_batch[:-1]\n",
    "y_test = test_batch[1:]\n",
    "\n",
    "test_hid = net.init_hidden()\n",
    "\n",
    "test_pred, _ = net.forward(x_test, test_hid)\n",
    "\n",
    "print(test_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_idx[word] = idx\n",
    "    idx_to_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny,Hnd.IapkeHixt.Mnynnly,\"Ther wnddwnwau.,\"pdey-v,kd,Moul!y, IulhHor[,rgk,ry.I waoxg.Hau.,Mf Ma whn..wngthe joteed,.wnd.Ia wf,,Iouue\"Iaich InddMox-hMhuwxvkeMfher,Hngtfd,\"ouegd,Hf Mherjulnl.ng.\"\"\"IWukMhiumov .Mn!.r.Ha wndenekyHn MherjuueexeHuxdkir,Ma wic\\rdwiv \"ere.wupnd,.Ifdthe jhec,, I#oe\"Tophk ,y,Hovydwou,,Iupt,Mn!.r.\"uexve-v ,Mnd.Iiic er,d,Ioued,eng Mhuwax,was ekd,\"\"\"IYWe Iox. \\nIavn.Hax Ihreggg Mhuwax IUaxwnddwexe.wapkeHfeechd.Mhuwour Irnt.Mold,r.\"Ior[bovlygg MngtisheIfl biic..Moumovkbn whuwop'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logprob_to_words(logprob, vocab):\n",
    "    \"\"\"given a sequence of logprobs from a network and a vocabulary, turn the logprob into words\n",
    "    \n",
    "    \"\"\"\n",
    "    seq_len = logprob.shape[0]\n",
    "    max_val, max_idx = logprob.max(dim=1)\n",
    "    txt = ''\n",
    "    for item in max_idx:\n",
    "        txt+=(idx_to_word[int(item)])\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "logprob_to_words(test_pred, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = torch.LongTensor([5,4,3,2])\n",
    "int(kk.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(100,20)\n",
    "\n",
    "out = embed(torch.LongTensor([2, 5, 0, 99, 34]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.LongTensor([2,5,4,3,1])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = embed(torch.rand(100,100).long())\n",
    "\n",
    "out.shape\n",
    "\n",
    "nn.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 'hello tofuboi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(g)):\n",
    "    print(g[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in g:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, char in enumerate(g):\n",
    "    print(idx,char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-0.4",
   "language": "python",
   "name": "pytorch0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
